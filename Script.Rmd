---
output: html_document
editor_options: 
  chunk_output_type: console
---

## Support Vector Machine (SVM)
### Contributors: Shakirah Nakalungi, Mark Okello
September 3, 2018

```{r setup, include=T, results='asis'}
knitr::opts_chunk$set(echo = T)
```

### Understanding SVM

SVM is a binary classifier which attempts to find a hyperplane that can separate two class of data points by the largest margin. 
We shall look at margins and  kernel trick which is the **most important** part of SVM that distincts SVM with other classifiers. According to ITSL book they explain SVM as a computer science way of 
doing thing since its the only classifier where you dont use probability.

![](figures/svmdifn.png)

In summary SVM is a machine learning technique that tries to separate data with a goal to find  the optimal separating hyperplane which maximizes the margin of the training data.

### Maximum Margin Classifier
Maximum Margin Classifier is the simple way to seperate(divide) your data if the training data is "linearly separable". It can not be applied to dataset which is not linearly seperable.

![***Figure showing maximum margin classifier***](figures/m_margin.png)


An hyperplane is a generalization of a plane.

1. in one dimension, an hyperplane is a point
2. in two dimensions its a line
3. in three dimensions its a plane
4. in morethan 3 dimensions its hard to visualize it. 

The fact there is a line separating the data points does not mean it is the best one.
![***illustrating the concept of which hyperplane to chose***](figures/whichhyperplane.png)

so our take home is that an optimal seperating hyperplane got by computing the distance between the hyperplane and the closest data point. Once we have this value, if we double it we get the margin.

### Support Vector Classifier
SVC is an extension to maximum margin classifier which allows some data points to be misclassified as shown below.

![***support vector classifier***](figures/no.png)

### Non Seperable Data

In some cases we are not able to use a soft margin (trying to seperate the data point while allowing failure for some data point to be in the right side but minimize the error on the fails). So in such cases we have to use ***appropriate*** kernel that transforms the input space to a high dimensional feature space where the data points can be seperable.
![***kernel trick***](figures/nonseparable.png)

There are many kernel functions but the common and a must know are:

1. linear: $K(x i , x j ) = x Ti x j$
2. polynomial: $K(x i , x j ) = (γx i T x j + r) d , γ > 0$
3. radial basis function (RBF): $K(x i , x j ) = exp(−γkx i − x j k 2 ), γ > 0$
4. sigmoid: $K(x i , x j ) = tanh(γx i T x j + r)$

---------------
